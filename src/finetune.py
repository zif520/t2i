import os
import argparse
import torch
import torch.nn.functional as F
from accelerate import Accelerator
from diffusers import DDPMScheduler, Transformer2DModel
from diffusers.optimization import get_scheduler
from tqdm.auto import tqdm

try:
    from src.config import config
    from src.data_loader import get_dataloader
    from src.model import get_text_encoder
except ImportError:
    from config import config
    from data_loader import get_dataloader
    from model import get_text_encoder

def finetune(pretrained_model_path):
    """
    SFT (Supervised Fine-Tuning) å¾®è°ƒè„šæœ¬
    
    æ¼”ç¤ºå¦‚ä½•åŠ è½½é¢„è®­ç»ƒå¥½çš„ DiT æ¨¡å‹ï¼Œå¹¶åœ¨å°æ•°æ®é›†ä¸Šç»§ç»­è®­ç»ƒã€‚
    """
    # 1. åˆå§‹åŒ– Accelerator
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=1, 
        log_with="all",
        project_dir=os.path.join(config.output_dir, "finetune_logs")
    )
    
    if accelerator.is_main_process:
        print(f"ğŸš€ å¼€å§‹ SFT å¾®è°ƒ! åŸºç¡€æ¨¡å‹: {pretrained_model_path}")

    # 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    # å…³é”®ç‚¹ï¼šæˆ‘ä»¬ä¸æ˜¯ä»å¤´åˆå§‹åŒ–ï¼Œè€Œæ˜¯åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æƒé‡: {pretrained_model_path}")
    model = Transformer2DModel.from_pretrained(pretrained_model_path)
    
    text_encoder = get_text_encoder()
    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)
    
    # 3. å¾®è°ƒè®¾ç½®
    # SFT é€šå¸¸ä½¿ç”¨æ›´ä½çš„å­¦ä¹ ç‡
    finetune_lr = 1e-5 
    optimizer = torch.optim.AdamW(model.parameters(), lr=finetune_lr)
    
    # è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ä¾ç„¶ä½¿ç”¨ Pokemon æ•°æ®é›†
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä½ å¯ä»¥æ›¿æ¢ä¸ºä½ çš„å‚ç›´é¢†åŸŸæ•°æ®é›† (å¦‚ "ä¸­å›½å±±æ°´ç”»" æ•°æ®é›†)
    train_dataloader = get_dataloader()
    
    lr_scheduler = get_scheduler(
        "constant", # å¾®è°ƒé€šå¸¸ä½¿ç”¨å¸¸æ•°å­¦ä¹ ç‡
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=(len(train_dataloader) * config.num_epochs)
    )

    # 4. Prepare
    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, lr_scheduler
    )
    text_encoder.to(accelerator.device)

    # 5. è®­ç»ƒå¾ªç¯ (ç®€åŒ–ç‰ˆ)
    # è¿™é‡Œçš„é€»è¾‘ä¸ train.py å®Œå…¨ä¸€è‡´
    model.train()
    for epoch in range(5): # å¾®è°ƒé€šå¸¸åªéœ€è¦å¾ˆå°‘çš„ Epoch
        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_main_process)
        progress_bar.set_description(f"Finetune Epoch {epoch}")
        
        for step, batch in enumerate(train_dataloader):
            clean_images = batch["pixel_values"]
            noise = torch.randn_like(clean_images)
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (clean_images.shape[0],), device=clean_images.device).long()
            
            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)
            
            with torch.no_grad():
                encoder_hidden_states = text_encoder(batch["input_ids"])[0]
            
            # åŒæ ·æ·»åŠ  dummy class labels
            dummy_class_labels = torch.zeros(clean_images.shape[0], dtype=torch.long, device=clean_images.device)
            
            model_pred = model(
                noisy_images, 
                timestep=timesteps, 
                encoder_hidden_states=encoder_hidden_states,
                class_labels=dummy_class_labels
            ).sample
            
            loss = F.mse_loss(model_pred, noise)
            
            accelerator.backward(loss)
            optimizer.step()
            optimizer.zero_grad()
            
            progress_bar.update(1)
            progress_bar.set_postfix(loss=loss.item())

    # ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
    if accelerator.is_main_process:
        save_path = os.path.join(config.output_dir, "finetuned-dit")
        accelerator.unwrap_model(model).save_pretrained(save_path)
        print(f"âœ… å¾®è°ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # é»˜è®¤åŠ è½½ output/pokemon-dit-64 ä¸‹çš„æœ€æ–° checkpoint (å‡è®¾ç”¨æˆ·å·²ç»è·‘äº† train.py)
    # å¦‚æœæ²¡æœ‰ï¼Œç”¨æˆ·éœ€è¦æ‰‹åŠ¨æŒ‡å®š
    default_path = os.path.join(config.output_dir, "checkpoint-epoch-50")
    parser.add_argument("--model_path", type=str, default=default_path, help="é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„")
    args = parser.parse_args()
    
    if not os.path.exists(args.model_path):
        print(f"âš ï¸ è­¦å‘Š: è·¯å¾„ {args.model_path} ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œ src/train.py æˆ–æŒ‡å®šæ­£ç¡®çš„è·¯å¾„ã€‚")
    else:
        finetune(args.model_path)
