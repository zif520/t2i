import torch
import argparse
import os
from diffusers import Transformer2DModel, DDPMScheduler
from transformers import CLIPTokenizer, CLIPTextModel
from torchvision.utils import save_image
from tqdm.auto import tqdm

try:
    from src.config import config
except ImportError:
    from config import config

def inference(prompt, model_path, guidance_scale=7.5, num_steps=50):
    """
    æ–‡ç”Ÿå›¾æ¨ç†è„šæœ¬
    
    Args:
        prompt: æ–‡æœ¬æç¤ºè¯
        model_path: æ¨¡å‹æƒé‡è·¯å¾„
        guidance_scale: CFG (Classifier-Free Guidance) å¼ºåº¦ï¼Œé€šå¸¸ 7.5 æ•ˆæœè¾ƒå¥½
        num_steps: é‡‡æ ·æ­¥æ•°
    """
    # 1. è®¾å¤‡é…ç½®
    device = torch.device(config.device)
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ğŸ¨ æç¤ºè¯: '{prompt}'")

    # 2. åŠ è½½æ¨¡å‹
    print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}")

    # åŠ è½½æˆ‘ä»¬è®­ç»ƒå¥½çš„ DiT æ¨¡å‹
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ Transformer2DModel.from_pretrained åŠ è½½
    # å¹¶ä¸”å¿…é¡»ç¡®ä¿é…ç½®ä¸è®­ç»ƒæ—¶ä¸€è‡´ (model.py ä¸­çš„ get_dit_model)
    
    # å°è¯•ç›´æ¥åŠ è½½
    # æ³¨æ„ï¼šå¦‚æœ Transformer2DModel å®ä¾‹åŒ–æ—¶æ²¡æœ‰æ­£ç¡®è¯†åˆ«ä¸ºæ”¯æŒ cross-attentionï¼Œ
    # å®ƒå¯èƒ½ä¸ä¼šæ¥å— encoder_hidden_statesã€‚
    # åœ¨ diffusers ä¸­ï¼ŒDiT é€šå¸¸é€šè¿‡ class labels æ§åˆ¶ï¼Œè€Œ Cross-Attention éœ€è¦ç‰¹å®šçš„é…ç½®ã€‚
    # ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬å¼ºåˆ¶ä½¿ç”¨ get_dit_model() æ„å»ºæ¨¡å‹ï¼Œç„¶ååŠ è½½æƒé‡ã€‚
    
    try:
        # å¼ºåˆ¶ä½¿ç”¨ä»£ç ä¸­å®šä¹‰çš„ç»“æ„ï¼Œç¡®ä¿é…ç½®æ­£ç¡® (åŒ…å« cross_attention_dim)
        from model import get_dit_model
        print("ğŸ—ï¸ ä½¿ç”¨ model.py å®šä¹‰æ„å»ºæ¨¡å‹ç»“æ„...")
        model = get_dit_model()
        
        # åŠ è½½æƒé‡
        from diffusers.models.modeling_utils import load_state_dict
        if os.path.isdir(model_path):
             # å°è¯•æŸ¥æ‰¾ safetensors
             weight_path = os.path.join(model_path, "diffusion_pytorch_model.safetensors")
             if not os.path.exists(weight_path):
                 weight_path = os.path.join(model_path, "diffusion_pytorch_model.bin")
        else:
             weight_path = model_path
             
        print(f"âš–ï¸ åŠ è½½æƒé‡: {weight_path}")
        state_dict = load_state_dict(weight_path)
        
        # è¿‡æ»¤æ‰ä¸åŒ¹é…çš„é”® (å¦‚æœæœ‰çš„è¯ï¼Œä¾‹å¦‚ unexpected keys)
        # model.load_state_dict(state_dict, strict=False) 
        # ä½¿ç”¨ strict=True ä»¥ç¡®ä¿æˆ‘ä»¬è®­ç»ƒçš„æƒé‡å®Œå…¨åŒ¹é…
        # å¦‚æœæŠ¥é”™ï¼Œè¯´æ˜ä¿å­˜çš„æƒé‡å’Œå½“å‰æ¨¡å‹å®šä¹‰ä¸ä¸€è‡´
        model.load_state_dict(state_dict)
        
    except Exception as e:
        print(f"âš ï¸ è‡ªå®šä¹‰åŠ è½½å¤±è´¥: {e}")
        print("å°è¯•å›é€€åˆ° from_pretrained...")
        model = Transformer2DModel.from_pretrained(model_path, use_safetensors=True)

    model.to(device)
    model.eval()
    
    # åŠ è½½ CLIP
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
    text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32").to(device)

    # 3. å‡†å¤‡æ–‡æœ¬ Embeddings (å« CFG å¤„ç†)
    # CFG éœ€è¦ä¸¤ä¸ªè¾“å…¥ï¼šæœ‰æç¤ºçš„ (Conditional) å’Œ ç©ºæç¤ºçš„ (Unconditional)
    
    # (a) Conditional Embeddings
    text_input = tokenizer(
        [prompt], padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt"
    )
    cond_embeddings = text_encoder(text_input.input_ids.to(device))[0]
    
    # (b) Unconditional Embeddings (ç©ºæ–‡æœ¬)
    uncond_input = tokenizer(
        [""], padding="max_length", max_length=tokenizer.model_max_length, return_tensors="pt"
    )
    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]
    
    # (c) æ‹¼æ¥ (Batch Size = 2)
    # ä¸ºäº†å¹¶è¡Œè®¡ç®—ï¼Œæˆ‘ä»¬å°†å®ƒä»¬æ‹¼åœ¨ä¸€èµ·
    text_embeddings = torch.cat([uncond_embeddings, cond_embeddings])

    # 4. åˆå§‹åŒ–å™ªå£°
    # ä»çº¯é«˜æ–¯å™ªå£°å¼€å§‹
    generator = torch.Generator(device=device).manual_seed(config.seed)
    latents = torch.randn(
        (1, 3, config.image_size, config.image_size),
        generator=generator,
        device=device
    )
    
    # 5. è®¾ç½® Scheduler
    # æ¨ç†æ—¶æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ›´å¿«çš„ Schedulerï¼Œè¿™é‡Œä¸ºäº†ç®€å•ä»ç”¨ DDPM
    scheduler = DDPMScheduler(num_train_timesteps=1000)
    scheduler.set_timesteps(num_steps)

    # 6. é‡‡æ ·å¾ªç¯ (Denoising Loop)
    print("âœ¨ å¼€å§‹ç”Ÿæˆ...")
    for t in tqdm(scheduler.timesteps):
        # 1. æ‰©å±• Latents ä»¥é€‚åº” CFG (Batch Size * 2)
        latent_model_input = torch.cat([latents] * 2)
        
        # 2. æ¨¡å‹é¢„æµ‹å™ªå£°
        # åŒæ ·éœ€è¦æ„é€  dummy class labels (2*BatchSize)
        dummy_class_labels = torch.zeros(latent_model_input.shape[0], dtype=torch.long, device=device)
        
        # ç¡®ä¿ timestep æ˜¯ 1D tensor
        # t åªæ˜¯ä¸€ä¸ªæ ‡é‡ (int æˆ– float)ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒæ‰©å±•æˆ (batch_size,)
        timestep_tensor = torch.tensor([t] * latent_model_input.shape[0], device=device)
        
        with torch.no_grad():
            noise_pred = model(
                latent_model_input, 
                timestep=timestep_tensor, 
                encoder_hidden_states=text_embeddings,
                class_labels=dummy_class_labels
            ).sample

        # 3. åº”ç”¨ CFG (Classifier-Free Guidance)
        # noise_pred åŒ…å«äº† [uncond_pred, cond_pred]
        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
        
        # æ ¸å¿ƒå…¬å¼: result = uncond + scale * (cond - uncond)
        # scale > 1 æ—¶ï¼Œä¼šå¼ºåŒ–æ–‡æœ¬å¯¹ç”Ÿæˆç»“æœçš„å½±å“
        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

        # 4. è®¡ç®—å‰ä¸€æ­¥çš„ Latents (å»å™ª)
        latents = scheduler.step(noise_pred, t, latents).prev_sample

    # 7. åå¤„ç†ä¸ä¿å­˜
    # [-1, 1] -> [0, 1]
    image = (latents / 2 + 0.5).clamp(0, 1)
    image = image.cpu()
    
    output_filename = f"generated_{prompt.replace(' ', '_')}.png"
    save_image(image, output_filename)
    print(f"âœ… å›¾ç‰‡å·²ä¿å­˜è‡³: {output_filename}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--prompt", type=str, default="a red pokemon", help="ç”Ÿæˆçš„æç¤ºè¯")
    
    # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„ checkpoint
    default_path = None
    if os.path.exists(config.output_dir):
        # åˆ—å‡ºæ‰€æœ‰ checkpoint æ–‡ä»¶å¤¹
        checkpoints = [d for d in os.listdir(config.output_dir) if d.startswith("checkpoint-epoch-")]
        if checkpoints:
            # æ’åºè§„åˆ™ï¼šæå– epoch æ•°å­—è¿›è¡Œæ’åº (checkpoint-epoch-1, checkpoint-epoch-2, ...)
            # å‡è®¾æ–‡ä»¶å¤¹æ ¼å¼ä¸¥æ ¼ä¸º checkpoint-epoch-N
            try:
                checkpoints.sort(key=lambda x: int(x.split("-")[-1]))
                default_path = os.path.join(config.output_dir, checkpoints[-1])
            except ValueError:
                # å¦‚æœæ ¼å¼ä¸å¯¹ï¼Œå°±æŒ‰å­—æ¯åº
                checkpoints.sort()
                default_path = os.path.join(config.output_dir, checkpoints[-1])
    
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå›é€€åˆ°é»˜è®¤çš„ checkpoint-epoch-50 (ç”¨äºæç¤ºç”¨æˆ·)
    if default_path is None:
        default_path = os.path.join(config.output_dir, "checkpoint-epoch-50")

    parser.add_argument("--model_path", type=str, default=default_path)
    
    args = parser.parse_args()
    
    if not os.path.exists(args.model_path):
         print(f"âš ï¸ è­¦å‘Š: æ¨¡å‹è·¯å¾„ {args.model_path} ä¸å­˜åœ¨ã€‚è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æ£€æŸ¥è·¯å¾„ã€‚")
         print(f"æç¤º: æ‚¨å¯ä»¥ä½¿ç”¨ --model_path æŒ‡å®šå…·ä½“è·¯å¾„ã€‚")
    else:
        inference(args.prompt, args.model_path)
