import torch
from torchvision import transforms
from datasets import load_dataset
from transformers import CLIPTokenizer
from torch.utils.data import DataLoader
# å¼•ç”¨æœ¬åœ°é…ç½®
try:
    from src.config import config
except ImportError:
    from config import config

class TextToImageDataset:
    """
    é€šç”¨æ–‡ç”Ÿå›¾æ•°æ®é›†åŠ è½½å™¨
    
    æ”¯æŒ:
    1. å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®é›† (å¦‚ Pokemon)
    2. å›¾åƒ-æ ‡ç­¾æ•°æ®é›† (å¦‚ CIFAR-10)ï¼Œä¼šè‡ªåŠ¨å°†æ ‡ç­¾è½¬æ¢ä¸ºæ–‡æœ¬æç¤º
    """
    def __init__(self):
        print(f"ğŸ“š æ­£åœ¨åŠ è½½ Tokenizer: openai/clip-vit-base-patch32 ...")
        self.tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
        
        # å›¾åƒå¢å¼ºä¸é¢„å¤„ç†
        self.transforms = transforms.Compose([
            transforms.Resize(config.image_size, interpolation=transforms.InterpolationMode.BILINEAR),
            transforms.CenterCrop(config.image_size),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ])

        # CIFAR-10 ç±»åˆ«æ˜ å°„
        self.cifar10_classes = {
            0: "airplane", 1: "automobile", 2: "bird", 3: "cat", 4: "deer",
            5: "dog", 6: "frog", 7: "horse", 8: "ship", 9: "truck"
        }
        
        # å†…éƒ¨çŠ¶æ€ï¼Œç”¨äº transform
        self.image_col = "image"
        self.text_col = "text"
        self.label_col = None

    def _transform_function(self, examples):
        """
        æ•°æ®è½¬æ¢å‡½æ•° (å¿…é¡»æ˜¯ picklable çš„ï¼Œä¸èƒ½æ˜¯å±€éƒ¨å‡½æ•°)
        """
        # 1. å¤„ç†å›¾åƒ
        pixel_values = [self.transforms(img.convert("RGB")) for img in examples[self.image_col]]
        
        # 2. å¤„ç†æ–‡æœ¬
        captions = []
        if self.text_col and self.text_col in examples:
            captions = examples[self.text_col]
        elif self.label_col and self.label_col in examples:
            # å¦‚æœæ˜¯åˆ†ç±»æ•°æ®é›†ï¼Œæ ¹æ® Label ç”Ÿæˆ Prompt
            labels = examples[self.label_col]
            for label in labels:
                if config.dataset_name == "cifar10":
                    class_name = self.cifar10_classes.get(label, "object")
                    captions.append(f"a photo of a {class_name}")
                else:
                    captions.append(f"a photo of class {label}")
        else:
            captions = [""] * len(pixel_values)

        # 3. Tokenize
        inputs = self.tokenizer(
            captions, 
            max_length=self.tokenizer.model_max_length, 
            padding="max_length", 
            truncation=True, 
            return_tensors="pt"
        )
        
        result = {
            "pixel_values": pixel_values,
            "input_ids": inputs.input_ids
        }
        
        # å¦‚æœæœ‰ labelï¼Œä¹Ÿè¿”å›å®ƒï¼Œç”¨äºè®­ç»ƒæ—¶çš„ Text Embedding ç¼“å­˜ä¼˜åŒ–
        if self.label_col and self.label_col in examples:
            result["labels"] = examples[self.label_col]
            
        return result

    def load_data(self):
        print(f"â¬‡ï¸ æ­£åœ¨åŠ è½½æ•°æ®é›†: {config.dataset_name} ...")
        dataset = load_dataset(config.dataset_name, split="train", cache_dir=config.dataset_cache_dir)
        
        # è¯†åˆ«æ•°æ®é›†åˆ—å
        column_names = dataset.column_names
        self.image_col = "image" if "image" in column_names else "img"
        self.text_col = "text" if "text" in column_names else None
        self.label_col = "label" if "label" in column_names else None
        
        print(f"ğŸ“‹ æ£€æµ‹åˆ°åˆ—å: {column_names}")
        print(f"   Imageåˆ—: {self.image_col}, Textåˆ—: {self.text_col}, Labelåˆ—: {self.label_col}")

        # ä½¿ç”¨ with_transform (set_transform) åŠ¨æ€å¤„ç†
        # ä¼ å…¥ bound method self._transform_function æ˜¯å¯ä»¥ picklable çš„
        dataset.set_transform(self._transform_function)
        
        return dataset

def get_dataloader():
    dataset_handler = TextToImageDataset()
    dataset = dataset_handler.load_data()
    
    dataloader = DataLoader(
        dataset,
        batch_size=config.train_batch_size,
        shuffle=True,
        num_workers=config.dataloader_num_workers, 
        persistent_workers=config.dataloader_persistent_workers,
        pin_memory=True
    )
    
    return dataloader

if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½æ¨¡å— (CIFAR-10 é€‚é…ç‰ˆ)...")
    loader = get_dataloader()
    
    try:
        batch = next(iter(loader))
        print(f"\nâœ… æ•°æ®åŠ è½½æˆåŠŸ!")
        print(f"ğŸ“¦ Image Batch Shape: {batch['pixel_values'].shape}")
        print(f"ğŸ“ Text Token Shape: {batch['input_ids'].shape}")
        
        # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ caption (åè§£ token)
        tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
        first_caption = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True)
        print(f"ğŸ” æ ·æœ¬ 0 æ–‡æœ¬: '{first_caption}'")
        
        # ä¿å­˜ä¸€å¼ æ ·æœ¬å›¾ç”¨äºéªŒè¯
        import torchvision
        img = batch['pixel_values'][0] * 0.5 + 0.5 # åå½’ä¸€åŒ–
        torchvision.utils.save_image(img, "sample_cifar10_resized.png")
        print(f"ğŸ–¼ï¸ å·²ä¿å­˜æ ·æœ¬å›¾: sample_cifar10_resized.png")
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
