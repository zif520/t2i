import torch
from torchvision import transforms
from datasets import load_dataset
from transformers import CLIPTokenizer
from torch.utils.data import DataLoader
# å¼•ç”¨æœ¬åœ°é…ç½®
try:
    from src.config import config
except ImportError:
    from config import config

class TextToImageDataset:
    """
    é€šç”¨æ–‡ç”Ÿå›¾æ•°æ®é›†åŠ è½½å™¨
    
    æ”¯æŒ:
    1. å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®é›† (å¦‚ Pokemon)
    2. å›¾åƒ-æ ‡ç­¾æ•°æ®é›† (å¦‚ CIFAR-10)ï¼Œä¼šè‡ªåŠ¨å°†æ ‡ç­¾è½¬æ¢ä¸ºæ–‡æœ¬æç¤º
    """
    def __init__(self):
        print(f"ğŸ“š æ­£åœ¨åŠ è½½ Tokenizer: openai/clip-vit-base-patch32 ...")
        self.tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
        
        # å›¾åƒå¢å¼ºä¸é¢„å¤„ç†
        self.transforms = transforms.Compose([
            transforms.Resize(config.image_size, interpolation=transforms.InterpolationMode.BILINEAR),
            transforms.CenterCrop(config.image_size),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ])

        # CIFAR-10 ç±»åˆ«æ˜ å°„
        self.cifar10_classes = {
            0: "airplane", 1: "automobile", 2: "bird", 3: "cat", 4: "deer",
            5: "dog", 6: "frog", 7: "horse", 8: "ship", 9: "truck"
        }
        
        # å†…éƒ¨çŠ¶æ€ï¼Œç”¨äº transform
        self.image_col = "image"
        self.text_col = "text"
        self.label_col = None

    def _transform_function(self, examples):
        """
        æ•°æ®è½¬æ¢å‡½æ•° (å¿…é¡»æ˜¯ picklable çš„ï¼Œä¸èƒ½æ˜¯å±€éƒ¨å‡½æ•°)
        """
        # 1. å¤„ç†å›¾åƒ
        pixel_values = [self.transforms(img.convert("RGB")) for img in examples[self.image_col]]
        
        # 2. å¤„ç†æ–‡æœ¬
        captions = []
        if self.text_col and self.text_col in examples:
            captions = examples[self.text_col]
        elif self.label_col and self.label_col in examples:
            # å¦‚æœæ˜¯åˆ†ç±»æ•°æ®é›†ï¼Œæ ¹æ® Label ç”Ÿæˆ Prompt
            labels = examples[self.label_col]
            for label in labels:
                if config.dataset_name == "cifar10":
                    class_name = self.cifar10_classes.get(label, "object")
                    captions.append(f"a photo of a {class_name}")
                else:
                    captions.append(f"a photo of class {label}")
        else:
            captions = [""] * len(pixel_values)

        # 3. Tokenize
        inputs = self.tokenizer(
            captions, 
            max_length=self.tokenizer.model_max_length, 
            padding="max_length", 
            truncation=True, 
            return_tensors="pt"
        )
        
        result = {
            "pixel_values": pixel_values,
            "input_ids": inputs.input_ids
        }
        
        # å¦‚æœæœ‰ labelï¼Œä¹Ÿè¿”å›å®ƒï¼Œç”¨äºè®­ç»ƒæ—¶çš„ Text Embedding ç¼“å­˜ä¼˜åŒ–
        if self.label_col and self.label_col in examples:
            result["labels"] = examples[self.label_col]
            
        return result

    def load_data(self):
        print(f"â¬‡ï¸ æ­£åœ¨åŠ è½½æ•°æ®é›†: {config.dataset_name} ...")
        dataset = load_dataset(config.dataset_name, split="train", cache_dir=config.dataset_cache_dir)
        
        # è¯†åˆ«æ•°æ®é›†åˆ—å
        column_names = dataset.column_names
        self.image_col = "image" if "image" in column_names else "img"
        self.text_col = "text" if "text" in column_names else None
        self.label_col = "label" if "label" in column_names else None
        
        print(f"ğŸ“‹ æ£€æµ‹åˆ°åˆ—å: {column_names}")
        print(f"   Imageåˆ—: {self.image_col}, Textåˆ—: {self.text_col}, Labelåˆ—: {self.label_col}")

        # ä½¿ç”¨ with_transform (set_transform) åŠ¨æ€å¤„ç†
        # ä¼ å…¥ bound method self._transform_function æ˜¯å¯ä»¥ picklable çš„
        dataset.set_transform(self._transform_function)
        
        # å†…å­˜ç¼“å­˜ä¼˜åŒ– (In-Memory Cache)
        if hasattr(config, "in_memory_cache") and config.in_memory_cache:
            if config.dataset_name == "cifar10":
                print("âš¡ï¸ æ­£åœ¨å°† CIFAR-10 å…¨é‡åŠ è½½åˆ°å†…å­˜ (In-Memory Cache) ...")
                # éå†ä¸€æ¬¡ datasetï¼Œè§¦å‘ transformï¼Œå¹¶å †å ä¸º Tensor
                # æ³¨æ„: è¿™ä¼šæ¶ˆè€—çº¦ 700MB å†…å­˜ï¼Œä½†ä¼šæå¤§åŠ é€Ÿè®­ç»ƒ
                # ç”±äº dataset è®¾ç½®äº† transformï¼Œè¿­ä»£å®ƒå°±ä¼šå¾—åˆ°å¤„ç†åçš„ Tensor
                
                # ä¸ºäº†åŠ é€Ÿï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ DataLoader å¤šè¿›ç¨‹æ¥åšè¿™ä¸ªé¢„åŠ è½½
                temp_loader = DataLoader(
                    dataset, 
                    batch_size=1000, 
                    num_workers=config.dataloader_num_workers,
                    shuffle=False
                )
                
                cached_data = []
                from tqdm import tqdm
                for batch in tqdm(temp_loader, desc="Caching Dataset"):
                    cached_data.append(batch)
                
                # æ„é€ ä¸€ä¸ªæ–°çš„ TensorDataset
                # æˆ‘ä»¬éœ€è¦åˆå¹¶ list of batches
                # batch æ˜¯ä¸€ä¸ª dict: {'pixel_values': Tensor, 'input_ids': Tensor, 'labels': Tensor}
                
                final_pixel_values = torch.cat([b["pixel_values"] for b in cached_data], dim=0)
                final_input_ids = torch.cat([b["input_ids"] for b in cached_data], dim=0)
                final_labels = torch.cat([b["labels"] for b in cached_data], dim=0) if "labels" in cached_data[0] else None
                
                print(f"âœ… ç¼“å­˜å®Œæˆ! Images: {final_pixel_values.shape}, RAMå ç”¨: {final_pixel_values.element_size() * final_pixel_values.numel() / 1e6:.2f} MB")
                
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„ TensorDatasetWrapper (å¿…é¡»æ˜¯å¯ picklable çš„ï¼Œæ‰€ä»¥å®šä¹‰åœ¨å¤–é¢)
                return InMemoryDataset(final_pixel_values, final_input_ids, final_labels)

        return dataset

# å¿…é¡»å®šä¹‰åœ¨æ¨¡å—çº§åˆ«ï¼Œä»¥ä¾¿ multiprocessing å¯ä»¥ pickle
class InMemoryDataset(torch.utils.data.Dataset):
    def __init__(self, pixel_values, input_ids, labels=None):
        self.pixel_values = pixel_values
        self.input_ids = input_ids
        self.labels = labels
        
    def __len__(self):
        return len(self.pixel_values)
    
    def __getitem__(self, idx):
        item = {
            "pixel_values": self.pixel_values[idx],
            "input_ids": self.input_ids[idx]
        }
        if self.labels is not None:
            item["labels"] = self.labels[idx]
        return item

def get_dataloader():
    dataset_handler = TextToImageDataset()
    dataset = dataset_handler.load_data()
    
    dataloader = DataLoader(
        dataset,
        batch_size=config.train_batch_size,
        shuffle=True,
        num_workers=config.dataloader_num_workers, 
        persistent_workers=config.dataloader_persistent_workers,
        pin_memory=True
    )
    
    return dataloader

if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½æ¨¡å— (CIFAR-10 é€‚é…ç‰ˆ)...")
    loader = get_dataloader()
    
    try:
        batch = next(iter(loader))
        print(f"\nâœ… æ•°æ®åŠ è½½æˆåŠŸ!")
        print(f"ğŸ“¦ Image Batch Shape: {batch['pixel_values'].shape}")
        print(f"ğŸ“ Text Token Shape: {batch['input_ids'].shape}")
        
        # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ caption (åè§£ token)
        tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
        first_caption = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True)
        print(f"ğŸ” æ ·æœ¬ 0 æ–‡æœ¬: '{first_caption}'")
        
        # ä¿å­˜ä¸€å¼ æ ·æœ¬å›¾ç”¨äºéªŒè¯
        import torchvision
        img = batch['pixel_values'][0] * 0.5 + 0.5 # åå½’ä¸€åŒ–
        torchvision.utils.save_image(img, "sample_cifar10_resized.png")
        print(f"ğŸ–¼ï¸ å·²ä¿å­˜æ ·æœ¬å›¾: sample_cifar10_resized.png")
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
