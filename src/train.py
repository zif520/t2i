import os
import torch
import torch.nn.functional as F
from accelerate import Accelerator
from diffusers import DDPMScheduler
from diffusers.optimization import get_scheduler
from tqdm.auto import tqdm

try:
    from src.config import config
    from src.data_loader import get_dataloader
    from src.model import get_dit_model, get_text_encoder
except ImportError:
    from config import config
    from data_loader import get_dataloader
    from model import get_dit_model, get_text_encoder

def train():
    # 1. åˆå§‹åŒ– Accelerator
    # Accelerator ä¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡ (CPU/MPS/CUDA) å’Œæ··åˆç²¾åº¦
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=1, 
        log_with="all",
        project_dir=os.path.join(config.output_dir, "logs")
    )
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if accelerator.is_main_process:
        os.makedirs(config.output_dir, exist_ok=True)
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ! è¾“å‡ºç›®å½•: {config.output_dir}")
        print(f"ğŸ’» è®¾å¤‡: {accelerator.device}")

    # 2. å‡†å¤‡ç»„ä»¶
    # å™ªå£°è°ƒåº¦å™¨ (Noise Scheduler): è´Ÿè´£åŠ å™ªå’Œå»å™ªçš„æ•°å­¦è®¡ç®—
    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)
    
    # æ¨¡å‹
    model = get_dit_model()
    
    # --- è‡ªåŠ¨æ£€æµ‹ Resume (ä¸­æ–­æ¢å¤) ---
    start_epoch = 0
    resume_path = None
    if os.path.exists(config.output_dir):
        checkpoints = [d for d in os.listdir(config.output_dir) if d.startswith("checkpoint-epoch-")]
        if checkpoints:
            # æŒ‰ epoch æ•°å­—æ’åº
            checkpoints.sort(key=lambda x: int(x.split("-")[-1]))
            latest_checkpoint = checkpoints[-1]
            resume_path = os.path.join(config.output_dir, latest_checkpoint)
            
            # è§£æå·²å®Œæˆçš„ Epoch
            start_epoch = int(latest_checkpoint.split("-")[-1])
            
            if start_epoch < config.num_epochs:
                print(f"ğŸ”„ æ£€æµ‹åˆ°ä¸­æ–­çš„è®­ç»ƒ: {latest_checkpoint}")
                print(f"ğŸ“¥ æ­£åœ¨ä» Epoch {start_epoch} æ¢å¤æƒé‡...")
                # åŠ è½½æƒé‡è¦†ç›–åŸæ¨¡å‹
                model = Transformer2DModel.from_pretrained(resume_path)
            else:
                print(f"âœ… æ£€æµ‹åˆ°è®­ç»ƒå·²å®Œæˆ (Epoch {start_epoch}/{config.num_epochs})ï¼Œè‹¥éœ€é‡æ–°è®­ç»ƒè¯·æ¸…ç† output ç›®å½•ã€‚")
                start_epoch = 0 # æˆ–è€…ç›´æ¥é€€å‡º? è¿™é‡Œè®©å®ƒä» 0 å¼€å§‹æˆ–è€…ä¿æŒå®ŒæˆçŠ¶æ€æ¯”è¾ƒå¥½ã€‚
                # å¦‚æœå·²ç»è·‘å®Œäº†ï¼Œå°±ä¸åŠ è½½äº†ï¼Œæˆ–è€…åŠ è½½äº†ä¹Ÿæ²¡ç”¨ï¼Œå› ä¸ºå¾ªç¯ä¸ä¼šæ‰§è¡Œã€‚
                # è®©ç”¨æˆ·å†³å®šå§ï¼Œè¿™é‡Œå‡è®¾ç”¨æˆ·æƒ³ç»§ç»­è·‘æˆ–è€…é‡è·‘ã€‚
                # å¦‚æœæ˜¯ fully trainedï¼Œrange(5, 5) æ˜¯ç©ºçš„ï¼Œç›´æ¥ç»“æŸã€‚
                
    text_encoder = get_text_encoder()
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)
    
    # æ•°æ®åŠ è½½å™¨
    train_dataloader = get_dataloader()
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = get_scheduler(
        "cosine",
        optimizer=optimizer,
        num_warmup_steps=config.lr_warmup_steps,
        num_training_steps=(len(train_dataloader) * config.num_epochs)
    )

    # 3. ä½¿ç”¨ Accelerator åŒ…è£…å¯¹è±¡
    # æ³¨æ„ï¼šText Encoder ä¸éœ€è¦åŒ…è£…ï¼Œå› ä¸ºå®ƒä¸å‚ä¸è®­ç»ƒ (å†»ç»“çŠ¶æ€)
    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, lr_scheduler
    )
    
    # å°† Text Encoder ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡
    text_encoder.to(accelerator.device)

    # 4. é¢„è®¡ç®— Text Embeddings (é’ˆå¯¹ CIFAR-10 ç­‰åˆ†ç±»æ•°æ®é›†çš„ä¼˜åŒ–)
    # å¦‚æœæ˜¯ CIFAR-10ï¼Œåªæœ‰ 10 ä¸ªå›ºå®šçš„ Promptï¼Œé¢„å…ˆè®¡ç®—å¯ä»¥æå¤§åŠ é€Ÿ
    cached_text_embeddings = None
    if config.dataset_name == "cifar10":
        print("âš¡ï¸ æ£€æµ‹åˆ° CIFAR-10 æ•°æ®é›†ï¼Œæ­£åœ¨é¢„è®¡ç®— Text Embeddings ä»¥åŠ é€Ÿè®­ç»ƒ...")
        cifar10_classes = {
            0: "airplane", 1: "automobile", 2: "bird", 3: "cat", 4: "deer",
            5: "dog", 6: "frog", 7: "horse", 8: "ship", 9: "truck"
        }
        captions = [f"a photo of a {cifar10_classes[i]}" for i in range(10)]
        
        # ä¸´æ—¶ Tokenizer (å› ä¸º dataloader é‡Œçš„ tokenizer ä¸å®¹æ˜“è·å–ï¼Œè¿™é‡Œé‡æ–°åŠ è½½ä¸€ä¸ªä¹Ÿæ²¡äº‹)
        from transformers import CLIPTokenizer
        tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
        
        inputs = tokenizer(
            captions, max_length=tokenizer.model_max_length, padding="max_length", truncation=True, return_tensors="pt"
        )
        
        with torch.no_grad():
            # ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = inputs.input_ids.to(accelerator.device)
            # [10, 77, 512]
            cached_text_embeddings = text_encoder(input_ids)[0]
        
        print(f"âœ… Text Embeddings é¢„è®¡ç®—å®Œæˆ! Shape: {cached_text_embeddings.shape}")

    # ä¼˜åŒ– 3: å†…å­˜æ ¼å¼ä¼˜åŒ– (Channels Last)
    # é€‚ç”¨äºå·ç§¯å±‚è¾ƒå¤šçš„ç½‘ç»œï¼Œåœ¨ GPU ä¸Šé€šå¸¸æ›´å¿« (MPS ä¹Ÿæœ‰ä¸€å®šæ”¶ç›Š)
    model = model.to(memory_format=torch.channels_last)
    
    # 5. è®­ç»ƒå¾ªç¯
    global_step = 0
    
    # å¦‚æœæ˜¯ Resumeï¼Œéœ€è¦å¿«è¿› LR Scheduler å’Œ global_step
    if start_epoch > 0:
        steps_per_epoch = len(train_dataloader)
        resume_step = start_epoch * steps_per_epoch
        global_step = resume_step
        print(f"â© æ­£åœ¨å¿«è¿› LR Scheduler åˆ° step {resume_step} ...")
        # æ³¨æ„ï¼šè¿™é‡Œç®€å•çš„å¾ªç¯ step å¯èƒ½æ¯”è¾ƒæ…¢ï¼Œä½†æœ€ç¨³å¥
        # å¯¹äº AdamW + Cosineï¼Œè¿™ä¸€æ­¥å¾ˆé‡è¦
        for _ in range(resume_step):
            lr_scheduler.step()
    
    for epoch in range(start_epoch, config.num_epochs):
        model.train()
        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_main_process)
        progress_bar.set_description(f"Epoch {epoch}")

        for step, batch in enumerate(train_dataloader):
            # ä¼˜åŒ–: ç¡®ä¿è¾“å…¥ä¹Ÿæ˜¯ channels_last
            clean_images = batch["pixel_values"].to(memory_format=torch.channels_last)
            
            # --- A. é‡‡æ ·å™ªå£° ---
            # ç”Ÿæˆä¸è¾“å…¥å›¾åƒå½¢çŠ¶ä¸€è‡´çš„é«˜æ–¯å™ªå£°
            noise = torch.randn_like(clean_images)
            
            # --- B. é‡‡æ ·æ—¶é—´æ­¥ ---
            # ä¸ºæ¯ä¸ªæ ·æœ¬éšæœºé€‰æ‹©ä¸€ä¸ªæ—¶é—´æ­¥ t (0 åˆ° 999)
            bsz = clean_images.shape[0]
            timesteps = torch.randint(
                0, noise_scheduler.config.num_train_timesteps, (bsz,), device=clean_images.device
            ).long()

            # --- C. å‰å‘åŠ å™ª (Forward Diffusion) ---
            # æ ¹æ®æ—¶é—´æ­¥ tï¼Œå°†å™ªå£°æ·»åŠ åˆ°å›¾åƒä¸Š
            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)

            # --- D. è·å–æ–‡æœ¬æ¡ä»¶ ---
            # æ€§èƒ½ä¼˜åŒ–ï¼šå¦‚æœæ˜¯ CIFAR-10 ä¸”æœ‰ç¼“å­˜ï¼Œç›´æ¥æŸ¥è¡¨
            if cached_text_embeddings is not None and "labels" in batch:
                # batch["labels"] æ˜¯ [Batch] çš„ tensor
                # ç›´æ¥ç´¢å¼•è·å–å¯¹åº”çš„ embeddings [Batch, 77, 512]
                encoder_hidden_states = cached_text_embeddings[batch["labels"]]
            else:
                # å¸¸è§„æµç¨‹ï¼šå®æ—¶è®¡ç®—
                with torch.no_grad():
                    # CLIP Text Encoder è¾“å‡ºçš„ hidden_states
                    encoder_hidden_states = text_encoder(batch["input_ids"])[0]

            # --- E. æ¨¡å‹é¢„æµ‹ ---
            # DiT é¢„æµ‹å™ªå£° (Predict the noise)
            # Hack: ä¼ å…¥ dummy class_labels ä»¥æ»¡è¶³ ada_norm_zero çš„è¦æ±‚
            # æˆ‘ä»¬ç”¨å…¨ 0 ä½œä¸º class labelï¼Œç›¸å½“äºæ¨¡å‹è®¤ä¸ºæ‰€æœ‰å›¾ç‰‡éƒ½å±äºåŒä¸€ä¸ª"ç±»åˆ«"
            dummy_class_labels = torch.zeros(bsz, dtype=torch.long, device=clean_images.device)
            
            # å¼€å¯æ¢¯åº¦ç´¯ç§¯ä¸Šä¸‹æ–‡ (è™½ç„¶è¿™é‡Œæ˜¯ 1ï¼Œä½†ä¿æŒè§„èŒƒ)
            with accelerator.accumulate(model):
                model_pred = model(
                    noisy_images, 
                    timestep=timesteps, 
                    encoder_hidden_states=encoder_hidden_states,
                    class_labels=dummy_class_labels
                ).sample

                # --- F. è®¡ç®— Loss ---
                # ç›®æ ‡æ˜¯é¢„æµ‹æ·»åŠ çš„é‚£ä¸ªå™ªå£°
                loss = F.mse_loss(model_pred, noise)

                # --- G. åå‘ä¼ æ’­ ---
                accelerator.backward(loss)
                
                # æ¢¯åº¦è£å‰ª (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(model.parameters(), 1.0)
                    
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad(set_to_none=True) # set_to_none=True ç•¥å¾®èŠ‚çœæ˜¾å­˜å’Œæ“ä½œ

            progress_bar.update(1)
            progress_bar.set_postfix(loss=loss.item())
            global_step += 1

        # æ¯ä¸ª Epoch ç»“æŸåä¿å­˜æ¨¡å‹
        if accelerator.is_main_process:
            # ä¿®æ”¹: æ¯ä¸ª Epoch éƒ½ä¿å­˜ï¼Œé˜²æ­¢æ„å¤–ä¸­æ–­ä¸¢å¤±è¿›åº¦
            save_path = os.path.join(config.output_dir, f"checkpoint-epoch-{epoch+1}")
            # ä¿å­˜ Unwrap åçš„æ¨¡å‹ (å»é™¤ DDP/MPS åŒ…è£…)
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(save_path)
            print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")

    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    train()
