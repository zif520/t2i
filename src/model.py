import torch
from diffusers import Transformer2DModel
from transformers import CLIPTextModel
try:
    from src.config import config
except ImportError:
    from config import config

def get_dit_model():
    """
    æ„å»º DiT (Diffusion Transformer) æ¨¡å‹
    
    ä½¿ç”¨ diffusers åº“ä¸­çš„ Transformer2DModelã€‚
    è¿™ç›¸å½“äºä¸€ä¸ªâ€œå›¾åƒç‰ˆçš„ BERTâ€ï¼š
    1. è¾“å…¥å›¾ç‰‡è¢«åˆ‡åˆ†ä¸º patches (ç±»ä¼¼å•è¯)ã€‚
    2. ç»è¿‡å¤šå±‚ Transformer å¤„ç†ã€‚
    3. è¾“å‡ºåŒæ ·å¤§å°çš„å™ªå£°é¢„æµ‹ã€‚
    """
    print(f"ğŸ—ï¸ æ­£åœ¨æ„å»º DiT æ¨¡å‹ (Patch Size={config.patch_size})...")
    
    model = Transformer2DModel(
        sample_size=config.image_size,      # è¾“å…¥å¤§å° 64x64
        patch_size=config.patch_size,       # Patch å¤§å° 4x4 -> åºåˆ—é•¿åº¦ = (64/4)^2 = 256
        in_channels=3,                      # è¾“å…¥é€šé“ RGB
        num_layers=config.num_layers,       # Transformer å±‚æ•°
        attention_head_dim=config.attention_head_dim,
        num_attention_heads=config.num_attention_heads,
        out_channels=3,                     # è¾“å‡ºé€šé“ (é¢„æµ‹å™ªå£°ï¼Œä¸è¾“å…¥å½¢çŠ¶ä¸€è‡´)
        
        # --- å…³é”®ï¼šæ¡ä»¶ç”Ÿæˆé…ç½® ---
        cross_attention_dim=512,            # æ–‡æœ¬ç‰¹å¾çš„ç»´åº¦ (CLIP-ViT-Base-Patch32 çš„è¾“å‡ºç»´åº¦æ˜¯ 512)
        # è¿™å…è®¸æ¨¡å‹åœ¨æ¯ä¸€å±‚ Transformer ä¸­é€šè¿‡ Cross-Attention "å…³æ³¨" æ–‡æœ¬æè¿°
        
        # DiT å¿…å¤‡å‚æ•°
        norm_type="ada_norm_zero",  # å½“ä½¿ç”¨ patch_size æ—¶ï¼Œé€šå¸¸ä½¿ç”¨ AdaLayerNormZero
        num_embeds_ada_norm=1000,   # å¿…é¡»åŒ¹é… DDPMScheduler çš„ num_train_timesteps
        
        # å…³é”®ä¿®å¤ï¼šæˆ‘ä»¬ä¸éœ€è¦ç±»åˆ«æ¡ä»¶ (class_labels)ï¼Œå› ä¸ºæ˜¯ Text-to-Image
        # AdaLayerNormZero é»˜è®¤éœ€è¦ class_labelsï¼Œè¿™é‡Œæˆ‘ä»¬éœ€è¦è®©å®ƒçŸ¥é“æˆ‘ä»¬ä¸ä¼  class_labels
        # ä½† diffusers çš„å®ç°ä¸­ï¼Œada_norm_zero å¼ºç»‘å®šäº† class embeddingã€‚
        # å®é™…ä¸Šï¼Œå¯¹äºçº¯æ–‡ç”Ÿå›¾ DiTï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨ norm_type="layer_norm" ä½† diffusers é™åˆ¶äº† patch_size å¿…é¡»é… ada_normã€‚
        # æ›¿ä»£æ–¹æ¡ˆï¼šæ„é€ ä¸€ä¸ª dummy class label æˆ–è€…ä½¿ç”¨ä¸åŒçš„ norm ç­–ç•¥ã€‚
        # æ›´ç®€å•çš„æ–¹æ¡ˆï¼šä¸ä½¿ç”¨ patch_size (å³ä¸ä½¿ç”¨ DiT)ï¼Œæˆ–è€…æ‰‹åŠ¨æ„é€  class_labelsã€‚
        # 
        # ä¸ºäº†è®© DiT è·‘é€šï¼Œæˆ‘ä»¬è¿™é‡Œä¼ å…¥ä¸€ä¸ªå‡çš„ class label (å…¨0) åœ¨ train.py ä¸­ï¼Œ
        # å¹¶åœ¨è¿™é‡Œè®¾ç½® class_embed_type="timestep" (ä½†è¿™ä¸è¢« Transformer2DModel æ”¯æŒ)
        # 
        # æ­£ç¡®å§¿åŠ¿ï¼šdiffusers çš„ Transformer2DModel åœ¨ patch_size æ¨¡å¼ä¸‹ä¸»è¦è®¾è®¡ç»™ Class-Conditioned ç”Ÿæˆ (å¦‚ DiT paper)ã€‚
        # å¯¹äº Text-Conditionedï¼Œé€šå¸¸ä¸ä½¿ç”¨ ada_norm_zero æˆ–è€…éœ€è¦ hackã€‚
        # 
        # Hack: æˆ‘ä»¬åœ¨ train.py ä¸­ä¼ å…¥ class_labels=Noneï¼Œä½†è¿™é‡Œå¿…é¡»å»æ‰å¯¹ class_labels çš„ä¾èµ–ã€‚
        # ç„¶è€Œ Transformer2DModel æºç å¼ºåˆ¶æ£€æŸ¥ num_embeds_ada_normã€‚
        # 
        # è®©æˆ‘ä»¬å°è¯•æ”¹ä¸º norm_type="layer_norm" å¹¶å»æ‰ patch_size (é€€åŒ–ä¸ºæ™®é€š Transformer)ï¼Œ
        # æˆ–è€…ä¿ç•™ patch_size ä½†åœ¨ train.py ä¸­ä¼ å…¥ dummy class_labelsã€‚
        # 
        # å†³å®šï¼šä¸ºäº†ä¿æŒ DiT ç‰¹æ€§ï¼Œæˆ‘ä»¬åœ¨ train.py ä¸­ä¼ å…¥ dummy class_labelsã€‚
    )
    return model

def get_text_encoder():
    """
    åŠ è½½é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ (CLIP)
    """
    print(f"ğŸ§  æ­£åœ¨åŠ è½½ Text Encoder: openai/clip-vit-base-patch32 ...")
    text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")
    
    # å†»ç»“ Text Encoder å‚æ•°
    # æˆ‘ä»¬åªè®­ç»ƒ DiTï¼Œä¸è®­ç»ƒ CLIPï¼Œè¿™æ ·å¯ä»¥èŠ‚çœå¤§é‡æ˜¾å­˜å’Œè®¡ç®—èµ„æº
    text_encoder.requires_grad_(False)
    
    return text_encoder

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹æ„å»º
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹æ¶æ„...")
    
    dit = get_dit_model()
    text_encoder = get_text_encoder()
    
    # ç»Ÿè®¡å‚æ•°é‡
    dit_params = sum(p.numel() for p in dit.parameters() if p.requires_grad)
    print(f"ğŸ“¦ DiT Trainable Params: {dit_params / 1e6:.2f} M (Million)")
    
    # æ¨¡æ‹Ÿä¸€æ¬¡å‰å‘ä¼ æ’­ (Forward Pass)
    # 1. æ¨¡æ‹Ÿå›¾åƒè¾“å…¥ (Batch=2, Channel=3, H=64, W=64)
    dummy_image = torch.randn(2, 3, config.image_size, config.image_size)
    # 2. æ¨¡æ‹Ÿæ—¶é—´æ­¥ (Timestep)
    dummy_timestep = torch.tensor([0, 100])
    # 3. æ¨¡æ‹Ÿæ–‡æœ¬ç‰¹å¾ (Batch=2, Seq=77, Dim=512)
    dummy_encoder_hidden_states = torch.randn(2, 77, 512)
    
    # 4. æ¨¡å‹é¢„æµ‹
    # è¾“å‡ºåº”è¯¥æ˜¯ [2, 3, 64, 64]
    output = dit(
        dummy_image, 
        timestep=dummy_timestep, 
        encoder_hidden_states=dummy_encoder_hidden_states
    ).sample
    
    print(f"âœ… Forward Pass æˆåŠŸ!")
    print(f"Input Shape: {dummy_image.shape}")
    print(f"Output Shape: {output.shape}")
    
    if output.shape == dummy_image.shape:
        print("ğŸ‰ ç»´åº¦åŒ¹é…ï¼Œæ¶æ„éªŒè¯é€šè¿‡ã€‚")
